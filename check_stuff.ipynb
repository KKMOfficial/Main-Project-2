{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class DynaBasedEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            main_env,\n",
    "            expert_traj,\n",
    "            reward_estimator = None,\n",
    "            render_mode: Optional[str] = None,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        # parent environment\n",
    "        self.main_env = main_env\n",
    "        # reference trajectory\n",
    "        # self.__indeces = 93+np.array([1,2,3,4,5,6,8,9,10,11,12,13,33,34,38,45,46,50]).reshape(-1)\n",
    "        self.__indeces = np.array([list(range(3*x,3*x+3,1)) for x in [2,7,19,26,10,16]]).reshape(-1)\n",
    "        self.expert_traj = expert_traj\n",
    "        target_bodies = self.expert_traj['body_positions'].reshape(-1,93)\n",
    "        target_joints = self.expert_traj['joints'].reshape(-1,56)\n",
    "        self.target_state = torch.tensor(np.concatenate((target_bodies,target_joints), axis=-1)).numpy()#[:,self.__indeces]\n",
    "        # modified spaces\n",
    "        self.action_space = spaces.Box(low=float('-1'), high=float('1'), shape=(56,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=float('-inf'), high=float('inf'), shape=(149,), dtype=np.float32)\n",
    "        self.render_mode = render_mode\n",
    "        self.reward_estimator = reward_estimator\n",
    "\n",
    "    def step(self, action):\n",
    "        # current state\n",
    "        current_state = np.concatenate((_utils.get_features(physics=self.main_env.dm_env.physics, walker=self.main_env.dm_env._task._walker, props=[])['body_positions'].reshape(-1),\n",
    "                      np.array(self.main_env.dm_env.physics.bind(self.main_env.dm_env._task._walker.mocap_joints).qpos)), axis=-1)\n",
    "\n",
    "        # observation, reward, terminated, truncated, info\n",
    "        _, rew, done, _ = self.main_env.step(action)\n",
    "\n",
    "        # trajectory tracking reward\n",
    "        _current_time = self.main_env.dm_env._task._time_step\n",
    "        \n",
    "        next_state = np.concatenate((_utils.get_features(physics=self.main_env.dm_env.physics, walker=self.main_env.dm_env._task._walker, props=[])['body_positions'].reshape(-1),\n",
    "                      np.array(self.main_env.dm_env.physics.bind(self.main_env.dm_env._task._walker.mocap_joints).qpos)), axis=-1)\n",
    "        \n",
    "        # original open ai error\n",
    "        # error_joints = np.max(np.abs(next_state[:93]-self.target_state[_current_time, :93]))\n",
    "        error_bodies = np.max(np.abs(next_state[self.__indeces]- self.target_state[_current_time, self.__indeces]))\n",
    "        # rew = -(0.5*error_bodies+0.5*error_joints)\n",
    "        rew = 1-error_bodies\n",
    "\n",
    "        # if self.reward_estimator is not None:\n",
    "        #     rew = rew + 0.1*self.reward_estimator(torch.tensor(current_state, dtype=torch.float32),torch.tensor(action, dtype=torch.float32)).detach().cpu().numpy().reshape((-1))\n",
    "\n",
    "        if (done is True) and (_current_time <= 204-1): rew = np.zeros_like(rew)\n",
    "\n",
    "        # rew = -(chebyshev(self.target_state[_current_time], next_state[self.__indeces])**2)\n",
    "\n",
    "        return next_state, rew, done, False, {\"TimeLimit.truncated\":None}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        _, done = self.main_env.reset(), False\n",
    "        return np.concatenate((_utils.get_features(physics=self.main_env.dm_env.physics, walker=self.main_env.dm_env._task._walker, props=[])['body_positions'].reshape(-1),\n",
    "                              np.array(self.main_env.dm_env.physics.bind(self.main_env.dm_env._task._walker.mocap_joints).qpos)), axis=-1), done\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        return self.main_env.render(\"rgb_array\")\n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        self.main_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.26.5, Python 3.10.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\4_Installed_Softwares\\mamba_forge\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "e:\\4_Installed_Softwares\\mamba_forge\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : meta-information updated successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\4_Installed_Softwares\\mamba_forge\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from sac_modified import SAC\n",
    "from mocapact.envs import tracking\n",
    "from dm_control.locomotion.tasks.reference_pose import types\n",
    "import numpy as np\n",
    "import torch, pickle\n",
    "from gym import spaces\n",
    "\n",
    "# region file path\n",
    "# path information\n",
    "root_folder = \"E:\\MoCAP\\MCDH\\\\root_1\"\n",
    "policy_model_path = f\"{root_folder}\\\\sac_model_3 - Copy.zip\"\n",
    "dynamic_model_path = f\"{root_folder}\\d2_060_035.pt\"\n",
    "reward_model_path = f\"{root_folder}\\\\r1_512.pt\"\n",
    "dynamic_model_backup_path = f\"{root_folder}\\d2_060_035_backup.pt\"\n",
    "reward_model_backup_path = f\"{root_folder}\\\\r1_64_backup.pt\"\n",
    "replay_buffer_path = f\"{root_folder}\\\\replay_buffer.pt\"\n",
    "dynamic_model_replay_buffer = f\"{root_folder}\\dyna_replay_buffer.pt\"\n",
    "logger_path = f\"{root_folder}\\logs\"\n",
    "# logger2_path = f\"{root_folder}\\logs2\"\n",
    "logger2_path = logger_path\n",
    "reference_trajectory_path = f\"{root_folder}/traj_info.np\"\n",
    "# endregion\n",
    "# expert info\n",
    "dataset = types.ClipCollection(ids=['CMU_075_09'], start_steps=[0], end_steps=[194])\n",
    "# referenct trajectory information\n",
    "with open(reference_trajectory_path, \"rb\") as f: reference_info = pickle.load(f)\n",
    "# environment\n",
    "env  = tracking.MocapTrackingGymEnv(dataset, task_kwargs=dict(ghost_offset=np.array([0., 0., 0.])),)\n",
    "denv = DynaBasedEnv(env, reference_info, None)\n",
    "# policy model\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=3*[1024], qf=3*[1024]),\n",
    "    activation_fn=torch.nn.ReLU,\n",
    ")\n",
    "lr_schedule = 3e-5\n",
    "format_strings = ['csv', 'tensorboard', 'stdout']\n",
    "model = SAC.load(policy_model_path, env=denv, )\n",
    "model.load_replay_buffer(replay_buffer_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.replay_buffer.sample(80,reward_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
