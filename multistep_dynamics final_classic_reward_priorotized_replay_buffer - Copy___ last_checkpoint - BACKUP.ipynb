{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.26.5, Python 3.10.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\4_Installed_Softwares\\mamba_forge\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "e:\\4_Installed_Softwares\\mamba_forge\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import copy, cv2, pickle, torch, gym, random\n",
    "from tqdm import tqdm\n",
    "from dm_control.locomotion.tasks.reference_pose import types\n",
    "from dm_control.locomotion.tasks.reference_pose import utils as _utils\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from mocapact import observables\n",
    "from mocapact.sb3 import utils\n",
    "from mocapact.envs import tracking\n",
    "\n",
    "from stable_baselines3 import TD3, SAC\n",
    "# from sac_modified import SAC\n",
    "\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import get_device\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym import spaces\n",
    "from typing import Optional\n",
    "expert_path = \"E:\\MoCAP\\MCDH\\data\\experts\\CMU_075_09-0-203\\eval_rsi\\model\"\n",
    "expert = utils.load_policy(expert_path, observables.TIME_INDEX_OBSERVABLES)\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import chebyshev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region Dynamic Model Parameters\n",
    "# ps_mode : state=[real_state, pos_state]\n",
    "# DYNAMIC : STATE_SIZE,25*ACTION_SIZE -> 25*POS_SIZE\n",
    "STATE_SIZE = 206\n",
    "POS_SIZE = 149\n",
    "ACTION_SIZE = 56\n",
    "NUM_OF_CELLS = 25\n",
    "# endregion\n",
    "\n",
    "\n",
    "class DynaBasedEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            main_env,\n",
    "            expert_traj,\n",
    "            reward_estimator = None,\n",
    "            keys = None,\n",
    "            observation_space = None,\n",
    "            render_mode: Optional[str] = None,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        # parent environment\n",
    "        self.main_env = main_env\n",
    "        # reference trajectory\n",
    "        self.expert_traj = expert_traj\n",
    "        target_bodies = self.expert_traj['body_positions'].reshape(-1,93)\n",
    "        target_joints = self.expert_traj['joints'].reshape(-1,ACTION_SIZE)\n",
    "        self.target_state = torch.tensor(np.concatenate((target_bodies,target_joints), axis=-1)).numpy()#[:,self.__indeces]\n",
    "        # modified spaces\n",
    "        self.action_space = spaces.Box(low=float('-1'), high=float('1'), shape=(ACTION_SIZE,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=float('-inf'), high=float('inf'), shape=(STATE_SIZE,), dtype=np.float32)\n",
    "        self.render_mode = render_mode\n",
    "        self.reward_estimator = reward_estimator\n",
    "        # core environment information\n",
    "        self.core_state = None\n",
    "        self._keys = keys\n",
    "        self._observation_space = observation_space\n",
    "\n",
    "    def transform_observation(self, observations):\n",
    "        obs = []\n",
    "        for k in self._keys:\n",
    "            if k in observations:\n",
    "                obs.append(observations[k])\n",
    "            else:\n",
    "                tmp = list(observations.values())[0]\n",
    "                shape = list(tmp.shape)\n",
    "                shape[-1] = self._observation_space[k].shape[0]\n",
    "                obs.append(torch.full(shape, torch.nan, device=tmp.device))\n",
    "        obs = np.concatenate(obs, axis=-1)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action, ps_mode=False):\n",
    "\n",
    "        # observation, reward, terminated, truncated, info\n",
    "        self.core_state, rew, done, _ = self.main_env.step(action)\n",
    "        \n",
    "        \n",
    "        # trajectory tracking reward\n",
    "        _current_time = self.main_env.dm_env._task._time_step\n",
    "        \n",
    "        next_state = np.concatenate((_utils.get_features(physics=self.main_env.dm_env.physics, walker=self.main_env.dm_env._task._walker, props=[])['body_positions'].reshape(-1),\n",
    "                    np.array(self.main_env.dm_env.physics.bind(self.main_env.dm_env._task._walker.mocap_joints).qpos)), axis=-1)\n",
    "        \n",
    "\n",
    "        if ps_mode:\n",
    "            return (self.transform_observation(self.core_state), next_state), rew, done, False, {\"TimeLimit.truncated\":None}\n",
    "\n",
    "        if not ps_mode:\n",
    "            \n",
    "            # rew = 0.9*rew + 0.1*self.reward_estimator(state=torch.from_numpy(self.transform_observation(self.core_state)), action=torch.from_numpy(action)).detach().cpu().numpy()\n",
    "            # original open ai error\n",
    "            # rew = 1.3*rew - 0.1*np.sum(np.max(np.einsum(\"i,ji->ji\",np.abs(next_state- self.target_state[_current_time]),mask), axis=1)) -0.2*np.mean(np.abs(self.core_state['walker/joints_vel']))\n",
    "\n",
    "            return self.transform_observation(self.core_state), rew, done, False, {\"TimeLimit.truncated\":None}\n",
    "        \n",
    "        \n",
    "\n",
    "    def reset(self, seed=None, options=None, ps_mode=False):\n",
    "        self.core_state, done = self.main_env.reset(), False\n",
    "        if ps_mode:\n",
    "            return [self.transform_observation(self.core_state),np.concatenate((\n",
    "                (_utils.get_features(physics=self.main_env.dm_env.physics, walker=self.main_env.dm_env._task._walker, props=[])['body_positions'].reshape(-1),\n",
    "                                np.array(self.main_env.dm_env.physics.bind(self.main_env.dm_env._task._walker.mocap_joints).qpos))\n",
    "            ), axis=-1)], done\n",
    "        else:\n",
    "            return self.transform_observation(self.core_state), done\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        return self.main_env.render(\"rgb_array\")\n",
    "          \n",
    "    def close(self):\n",
    "        self.main_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([\n",
    "       [ 0.16101618, -0.40060299,  1.08903083],\n",
    "       [ 0.29373521, -0.4193849 ,  0.97654074],\n",
    "       [ 0.43153917, -0.52788541,  0.52336356],\n",
    "       [ 0.51678509, -0.3457796 ,  0.06685116],\n",
    "       [ 0.58474457, -0.51940714,  0.0323095 ],\n",
    "       [ 0.16101618, -0.40060299,  1.08903083],\n",
    "       [ 0.05369995, -0.46572327,  0.96711407],\n",
    "       [-0.00865177, -0.63607209,  0.51630879],\n",
    "       [-0.11421121, -0.40321956,  0.08797496],\n",
    "       [-0.12202077, -0.58772649,  0.04491287],\n",
    "       [ 0.16101618, -0.40060299,  1.08903083],\n",
    "       [ 0.15931813, -0.41555789,  1.22439721],\n",
    "       [ 0.16280991, -0.43823503,  1.35872984],\n",
    "       [ 0.17017723, -0.4625986 ,  1.49319125],\n",
    "       [ 0.16344592, -0.49144109,  1.59850858],\n",
    "       [ 0.18221923, -0.49999375,  1.75323714],\n",
    "       [ 0.30875609, -0.55831932,  1.76740717],\n",
    "       [ 0.17017723, -0.4625986 ,  1.49319125],\n",
    "       [ 0.37878143, -0.40217638,  1.59958084],\n",
    "       [ 0.41094637, -0.34053994,  1.27472563],\n",
    "       [ 0.54416648, -0.42036814,  1.12196594],\n",
    "       [ 0.61077624, -0.46028207,  1.04558644],\n",
    "       [ 0.66472814, -0.5038426 ,  0.99001949],\n",
    "       [ 0.61855088, -0.47893582,  1.06775833],\n",
    "       [ 0.17017723, -0.4625986 ,  1.49319125],\n",
    "       [-0.02078486, -0.57638904,  1.58843049],\n",
    "       [-0.14163758, -0.60054813,  1.27992522],\n",
    "       [-0.21789441, -0.74652476,  1.13733707],\n",
    "       [-0.25602266, -0.81951275,  1.06604331],\n",
    "       [-0.2897067 , -0.84907655,  0.98918458],\n",
    "       [-0.25383986, -0.84774668,  1.07594689]])\n",
    "\n",
    "# points[4][2] += 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           10,
           11,
           12,
           13,
           14,
           15,
           16,
           17,
           18,
           19,
           20,
           21,
           22,
           23,
           24,
           25,
           26,
           27,
           28,
           29,
           30
          ],
          "colorscale": [
           [
            0,
            "#440154"
           ],
           [
            0.1111111111111111,
            "#482878"
           ],
           [
            0.2222222222222222,
            "#3e4989"
           ],
           [
            0.3333333333333333,
            "#31688e"
           ],
           [
            0.4444444444444444,
            "#26828e"
           ],
           [
            0.5555555555555556,
            "#1f9e89"
           ],
           [
            0.6666666666666666,
            "#35b779"
           ],
           [
            0.7777777777777778,
            "#6ece58"
           ],
           [
            0.8888888888888888,
            "#b5de2b"
           ],
           [
            1,
            "#fde725"
           ]
          ],
          "opacity": 0.8,
          "size": 5
         },
         "mode": "markers+text",
         "text": [
          "0",
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          ">>>10",
          ">>>11",
          ">>>12",
          ">>>13",
          ">>>14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30"
         ],
         "textposition": "top center",
         "type": "scatter3d",
         "x": [
          0.16101618,
          0.29373521,
          0.43153917,
          0.51678509,
          0.58474457,
          0.16101618,
          0.05369995,
          -0.00865177,
          -0.11421121,
          -0.12202077,
          0.16101618,
          0.15931813,
          0.16280991,
          0.17017723,
          0.16344592,
          0.18221923,
          0.30875609,
          0.17017723,
          0.37878143,
          0.41094637,
          0.54416648,
          0.61077624,
          0.66472814,
          0.61855088,
          0.17017723,
          -0.02078486,
          -0.14163758,
          -0.21789441,
          -0.25602266,
          -0.2897067,
          -0.25383986
         ],
         "y": [
          -0.40060299,
          -0.4193849,
          -0.52788541,
          -0.3457796,
          -0.51940714,
          -0.40060299,
          -0.46572327,
          -0.63607209,
          -0.40321956,
          -0.58772649,
          -0.40060299,
          -0.41555789,
          -0.43823503,
          -0.4625986,
          -0.49144109,
          -0.49999375,
          -0.55831932,
          -0.4625986,
          -0.40217638,
          -0.34053994,
          -0.42036814,
          -0.46028207,
          -0.5038426,
          -0.47893582,
          -0.4625986,
          -0.57638904,
          -0.60054813,
          -0.74652476,
          -0.81951275,
          -0.84907655,
          -0.84774668
         ],
         "z": [
          1.08903083,
          0.97654074,
          0.52336356,
          0.06685116,
          0.0323095,
          1.08903083,
          0.96711407,
          0.51630879,
          0.08797496,
          0.04491287,
          1.08903083,
          1.22439721,
          1.35872984,
          1.49319125,
          1.59850858,
          1.75323714,
          1.76740717,
          1.49319125,
          1.59958084,
          1.27472563,
          1.12196594,
          1.04558644,
          0.99001949,
          1.06775833,
          1.49319125,
          1.58843049,
          1.27992522,
          1.13733707,
          1.06604331,
          0.98918458,
          1.07594689
         ]
        }
       ],
       "layout": {
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 0
        },
        "scene": {
         "xaxis": {
          "title": {
           "text": "X"
          }
         },
         "yaxis": {
          "title": {
           "text": "Y"
          }
         },
         "zaxis": {
          "title": {
           "text": "Z"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming 'points' is a tensor with shape (40, 3)\n",
    "x = points[:, 0]\n",
    "y = points[:, 1]\n",
    "z = points[:, 2]\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    text=[str(i) if i not in list(range(10,15)) else f\">>>{i}\" for i in range(len(x))],  # Convert indices to strings\n",
    "    mode='markers+text',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=np.arange(len(x)),\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    textposition='top center'\n",
    ")])\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z'\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# 2,7,12,20,27\n",
    "# 3x,3x+2\n",
    "# [2,7,19,26,10,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def r_to_i(x,y): return np.array([z for z in range(3*x,3*y)])\n",
    "# # np.max(d[1:5])\n",
    "# # np.max(d[6:10])\n",
    "# # np.max(d[18:24])\n",
    "# # np.max(d[25:31])\n",
    "# # np.max(d[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Dynamics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiStepDynamics(\n",
      "  (cells): ModuleList(\n",
      "    (0): T_Hat_(\n",
      "      (fc1): Linear(in_features=262, out_features=150, bias=True)\n",
      "      (fc2): Linear(in_features=150, out_features=130, bias=True)\n",
      "      (fc3): Linear(in_features=130, out_features=206, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1-24): 24 x T_Hat_(\n",
      "      (fc1): Linear(in_features=205, out_features=120, bias=True)\n",
      "      (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=206, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=206, out_features=149, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:57: DeprecationWarning: invalid escape sequence '\\M'\n",
      "<>:57: DeprecationWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_9128\\2469199770.py:57: DeprecationWarning: invalid escape sequence '\\M'\n",
      "  torch.save(d1.state_dict(), \"E:\\MoCAP\\MCDH\\\\root_1\\d1_init.pt\")\n"
     ]
    }
   ],
   "source": [
    "# region Dynamic Model Functions\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, device=torch.device(\"cpu\")):\n",
    "        self.X = torch.tensor(np.array(X), dtype=torch.float).to(device)  # Convert input features to tensors\n",
    "        self.y = torch.tensor(np.array(y), dtype=torch.float).to(device)  # Convert labels to tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        return x, label\n",
    "\n",
    "class T_Hat_(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_frac, const=100, dropout_= 0.2):\n",
    "        super(T_Hat_, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        hidden_layer1 = 10*(int(const+hidden_layer_frac[0]*input_size)//10)\n",
    "        hidden_layer2 = 10*(int(const+hidden_layer_frac[1]*input_size)//10)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer1)\n",
    "        self.fc2 = nn.Linear(hidden_layer1, hidden_layer2)\n",
    "        self.fc3 = nn.Linear(hidden_layer2, STATE_SIZE)\n",
    "        self.dropout = nn.Dropout(dropout_)\n",
    "        self.float()\n",
    "    def forward(self, x):\n",
    "        # observation bounds are (-inf, inf)\n",
    "        return torch.tanh(self.fc3(self.dropout(torch.tanh(self.fc2(self.dropout(torch.tanh(self.fc1(x))))))))\n",
    "    \n",
    "class MultiStepDynamics(nn.Module):\n",
    "    def __init__(self, num_of_cells, hidden_layer_frac, dropout=0.4):\n",
    "        super(MultiStepDynamics, self).__init__()\n",
    "        self.cells = nn.ModuleList().float()\n",
    "        self.input_size = [STATE_SIZE+ACTION_SIZE]+[POS_SIZE+ACTION_SIZE for _ in range(num_of_cells-1)]\n",
    "        for i in range(num_of_cells) : self.cells.append(T_Hat_(self.input_size[i], hidden_layer_frac, dropout))\n",
    "        self.decoder = nn.Linear(STATE_SIZE,POS_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is full state : S0+25A -> 25$\n",
    "        outputs = []\n",
    "        states = torch.tensor(x[:,:STATE_SIZE])\n",
    "        for i,sub_net in enumerate(self.cells):\n",
    "            next_state = sub_net(torch.cat((states, x[:,STATE_SIZE+i*ACTION_SIZE:STATE_SIZE+(i+1)*ACTION_SIZE]), dim=1))\n",
    "            next_state = self.decoder(next_state)\n",
    "            states = torch.cat((states,next_state), dim=1)[:,-POS_SIZE:]\n",
    "            outputs.append(next_state)\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "    def Nthforward(self, x, N):\n",
    "        # x is partial state : N'th module length is S+A if N=0 else 2*S+A\n",
    "        return self.cells[N](x)\n",
    "# endregion\n",
    "\n",
    "# region Dynamic Model Testing\n",
    "d1 = MultiStepDynamics(NUM_OF_CELLS,[0.6,0.5], dropout=0.2)\n",
    "torch.save(d1.state_dict(), \"E:\\MoCAP\\MCDH\\\\root_1\\d1_init.pt\")\n",
    "\n",
    "# for net in d1.cells:\n",
    "print(d1)\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiStepDynamics(\n",
      "  (cells): ModuleList(\n",
      "    (0): T_Hat_(\n",
      "      (fc1): Linear(in_features=262, out_features=150, bias=True)\n",
      "      (fc2): Linear(in_features=150, out_features=130, bias=True)\n",
      "      (fc3): Linear(in_features=130, out_features=206, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1-24): 24 x T_Hat_(\n",
      "      (fc1): Linear(in_features=205, out_features=120, bias=True)\n",
      "      (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
      "      (fc3): Linear(in_features=100, out_features=206, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=206, out_features=149, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:44: DeprecationWarning: invalid escape sequence '\\M'\n",
      "<>:44: DeprecationWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_9128\\851085720.py:44: DeprecationWarning: invalid escape sequence '\\M'\n",
      "  torch.save(d1.state_dict(), \"E:\\MoCAP\\MCDH\\\\root_1\\d1_init.pt\")\n"
     ]
    }
   ],
   "source": [
    "# transformers dynamic with special tokens and attention head\n",
    "class T_Hat_(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_frac, const=100, dropout_= 0.2):\n",
    "        super(T_Hat_, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        hidden_layer1 = 10*(int(const+hidden_layer_frac[0]*input_size)//10)\n",
    "        hidden_layer2 = 10*(int(const+hidden_layer_frac[1]*input_size)//10)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer1)\n",
    "        self.fc2 = nn.Linear(hidden_layer1, hidden_layer2)\n",
    "        self.fc3 = nn.Linear(hidden_layer2, STATE_SIZE)\n",
    "        self.dropout = nn.Dropout(dropout_)\n",
    "        self.float()\n",
    "    def forward(self, x):\n",
    "        # observation bounds are (-inf, inf)\n",
    "        return torch.tanh(self.fc3(self.dropout(torch.tanh(self.fc2(self.dropout(torch.tanh(self.fc1(x))))))))\n",
    "    \n",
    "class MultiStepDynamics(nn.Module):\n",
    "    def __init__(self, num_of_cells, hidden_layer_frac, dropout=0.4):\n",
    "        super(MultiStepDynamics, self).__init__()\n",
    "        self.cells = nn.ModuleList().float()\n",
    "        self.input_size = [STATE_SIZE+ACTION_SIZE]+[POS_SIZE+ACTION_SIZE for _ in range(num_of_cells-1)]\n",
    "        for i in range(num_of_cells) : self.cells.append(T_Hat_(self.input_size[i], hidden_layer_frac, dropout))\n",
    "        self.decoder = nn.Linear(STATE_SIZE,POS_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is full state : S0+25A -> 25$\n",
    "        outputs = []\n",
    "        states = torch.tensor(x[:,:STATE_SIZE])\n",
    "        for i,sub_net in enumerate(self.cells):\n",
    "            next_state = sub_net(torch.cat((states, x[:,STATE_SIZE+i*ACTION_SIZE:STATE_SIZE+(i+1)*ACTION_SIZE]), dim=1))\n",
    "            next_state = self.decoder(next_state)\n",
    "            states = torch.cat((states,next_state), dim=1)[:,-POS_SIZE:]\n",
    "            outputs.append(next_state)\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "    def Nthforward(self, x, N):\n",
    "        # x is partial state : N'th module length is S+A if N=0 else 2*S+A\n",
    "        return self.cells[N](x)\n",
    "# endregion\n",
    "\n",
    "# region Dynamic Model Testing\n",
    "d1 = MultiStepDynamics(NUM_OF_CELLS,[0.6,0.5], dropout=0.2)\n",
    "torch.save(d1.state_dict(), \"E:\\MoCAP\\MCDH\\\\root_1\\d1_init.pt\")\n",
    "\n",
    "# for net in d1.cells:\n",
    "print(d1)\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first run main training cell!\n",
    "Train_Dynamics        = True\n",
    "Collect_Expert_Trajs  = True\n",
    "TRAIN_DYNAMICS_INITIALIZATION_ONLY = True\n",
    "d1 = MultiStepDynamics(NUM_OF_CELLS, hidden_layer_frac=[0.6,0.5], dropout=0.2)\n",
    "# d1.load_state_dict(torch.load(\"E:/MoCAP/MCDH/root_1/d1_init.pt\"))\n",
    "# with open(\"E:/MoCAP/MCDH/root_1/dyna_replay_buffer_backup.pt\", \"rb\") as f:\n",
    "#     dynamic_dataset = pickle.load(f)\n",
    "dynamic_dataset=[[],[]]\n",
    "\n",
    "_sub_episode_num = 0\n",
    "_episode_num_bias = 0\n",
    "\n",
    "for episode_num in range(0,40*30,40):\n",
    "    # region Collect Trajectories\n",
    "    episode_num += _episode_num_bias\n",
    "    # truncate the dataset\n",
    "    dynamic_dataset[0]=dynamic_dataset[0][-2000:]\n",
    "    dynamic_dataset[1]=dynamic_dataset[1][-2000:]\n",
    "    \n",
    "    if Collect_Expert_Trajs:\n",
    "        for traj_num in tqdm(range(100)):\n",
    "\n",
    "            # recording video of robot performance\n",
    "            _current_eps_frames = []\n",
    "\n",
    "            n_state, done = denv.reset(ps_mode=True)\n",
    "            real_states, pos_states, actions = [], [], []\n",
    "            episode_start = copy.deepcopy(denv.main_env.dm_env._task._time_step)\n",
    "            while not done:\n",
    "                # capture current frame\n",
    "                _current_eps_frames += [denv.render()]\n",
    "                action = expert.predict(denv.core_state, deterministic=True)[0]\n",
    "                real_states += [n_state[0]]\n",
    "                pos_states += [n_state[1]]\n",
    "                actions += [action]\n",
    "                # FIX: n_state, reward, done, _, _ = denv.step(action.float().detach().numpy())\n",
    "                n_state, reward, done, _, _ = denv.step(action, ps_mode=True)\n",
    "            \n",
    "            # Update Dynamics Dataset\n",
    "            real_states += [n_state[0]]\n",
    "            pos_states += [n_state[1]]\n",
    "            if len(real_states) >= 25:\n",
    "                for start in range(len(real_states)-25+1-1):\n",
    "                    dynamic_dataset[0] += [ np.concatenate([real_states[start]]+actions[start:start+25], axis=-1) ]\n",
    "                    dynamic_dataset[1] += [ np.concatenate(pos_states[start+1:start+1+25], axis=-1) ]\n",
    "            else:\n",
    "                for start in range(len(real_states)):\n",
    "                    dynamic_dataset[0] += [ np.concatenate([real_states[start]]+actions[start:]+Z[:25-len(actions[start:])], axis=-1) ]\n",
    "                    dynamic_dataset[1] += [ np.concatenate(pos_states[start+1:]+[pos_states[-1] for _ in range(25-len(pos_states[start+1:]))], axis=-1) ]\n",
    "    # endregion\n",
    "\n",
    "    # region Train Dynamics\n",
    "    if Train_Dynamics:\n",
    "        for sub_episode_num in range(20):\n",
    "            # Create an instance of the custom dataset\n",
    "            indeces = random.sample(range(len(dynamic_dataset[1])), 2000)\n",
    "            dataset = CustomDataset([dynamic_dataset[0][__i] for __i in indeces], [dynamic_dataset[1][__i] for __i in indeces], device=torch.device(\"cuda\"))\n",
    "            # Train Dynamics\n",
    "            training_function(dataset, 1024, num_epochs, d1, writer, dynamic_start_state, _sub_episode_num+sub_episode_num, device=torch.device(\"cuda\"))\n",
    "            torch.save(d1.state_dict(), \"E:/MoCAP/MCDH/root_1/d1_init.pt\")\n",
    "            print(f\"{sub_episode_num}: [INFO] : Dynamics Model Stored Successfully.\")\n",
    "        _sub_episode_num += sub_episode_num\n",
    "    # endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Reward Via Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the IRL dataset\n",
    "class IRLDataset(Dataset):\n",
    "    def __init__(self, expert_states, expert_actions, agent_states, agent_actions):\n",
    "        self.expert_states  = expert_states\n",
    "        self.expert_actions = expert_actions\n",
    "        self.agent_states   = agent_states\n",
    "        self.agent_actions  = agent_actions\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(self.expert_states.shape[0],self.agent_states.shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.expert_states[idx], self.expert_actions[idx], self.agent_states[idx], self.agent_actions[idx])\n",
    "\n",
    "# Define discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, writer=None):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim, dtype=torch.float32)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1, dtype=torch.float32)\n",
    "        self.float()\n",
    "        self.writer = writer\n",
    "        self.steps = 0\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return torch.sigmoid(self.fc2(torch.relu(self.fc1(torch.cat([state, action], dim=-1)))))\n",
    "\n",
    "    def train(self, expert_data, agent_data, num_epochs=32, batch_size=32, threshold=None, lr=1e-3):\n",
    "        dataset = IRLDataset(expert_data[0], expert_data[1], agent_data[0], agent_data[1])\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        disc_optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.cpu()\n",
    "        for _ in tqdm(list(range(num_epochs))):\n",
    "            for expert_state, expert_action, agent_state, agent_action in dataloader:\n",
    "                expert_state, expert_action, agent_state, agent_action = expert_state.squeeze().cpu(), expert_action.squeeze().cpu(), agent_state.squeeze().cpu(), agent_action.squeeze().cpu()\n",
    "                disc_optimizer.zero_grad()\n",
    "                expert_output = self(expert_state, expert_action)\n",
    "                agent_output  = self(agent_state, agent_action)\n",
    "                disc_loss = -(torch.log(expert_output) + torch.log(1 - agent_output)).mean()\n",
    "                disc_loss.backward()\n",
    "                disc_optimizer.step()\n",
    "                if self.writer is not None:\n",
    "                    self.writer.add_scalar(\"reward discriminator loss\", disc_loss.item(), self.steps)\n",
    "                    self.steps += 1\n",
    "                if threshold is not None:\n",
    "                    if disc_loss.item() < threshold: return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model Along Sequential Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DYNAMICS_INITIALIZATION_ONLY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:31: DeprecationWarning: invalid escape sequence '\\M'\n",
      "<>:38: DeprecationWarning: invalid escape sequence '\\d'\n",
      "<>:39: DeprecationWarning: invalid escape sequence '\\l'\n",
      "<>:31: DeprecationWarning: invalid escape sequence '\\M'\n",
      "<>:38: DeprecationWarning: invalid escape sequence '\\d'\n",
      "<>:39: DeprecationWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_9128\\4257556344.py:31: DeprecationWarning: invalid escape sequence '\\M'\n",
      "  root_folder                 = \"E:\\MoCAP\\MCDH\\\\root_1\"\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_9128\\4257556344.py:38: DeprecationWarning: invalid escape sequence '\\d'\n",
      "  dynamic_model_replay_buffer = f\"{root_folder}\\dyna_replay_buffer_backup.pt\"\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_9128\\4257556344.py:39: DeprecationWarning: invalid escape sequence '\\l'\n",
      "  logger_path                 = f\"{root_folder}\\logs\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : meta-information updated successfully.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "[INFO] : Reward Initialized Successfully.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "[INFO] : Model Loaded Successfully.\n",
      "[INFO] : Buffer Loaded Successfully.\n",
      "[INFO] : Buffer Size IS 171417\n",
      "[INFO] : Physics Loaded Successfully.\n",
      "[INFO] : Physics Buffer Initialized Successfully.\n",
      "Logging to E:\\MoCAP\\MCDH\\root_1\\logs\n",
      "[INFO] : Environment Reward Estimator Was Set Successfully.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "STOP HERE",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 288\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] : Environment Reward Estimator Was Set Successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# endregion\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# endregion\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTOP HERE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# region algorithm main loop\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discriminator_warm_up_phase:\n",
      "\u001b[1;31mException\u001b[0m: STOP HERE"
     ]
    }
   ],
   "source": [
    "# region Important Notes\n",
    "# most important parameters to set\n",
    "# training rate 3e-4\n",
    "# depth and width of the value and policy networks 3*1024\n",
    "# entropy coefficient 0.7\n",
    "# make the buffer size 100K\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# endregion\n",
    "\n",
    "# region control flags\n",
    "# training initialization or storing\n",
    "INITIALIZE_TRAINING_INSTANCE = False\n",
    "load_replay_buffer = True\n",
    "load_physics = True\n",
    "load_reward = False\n",
    "load_networks = True\n",
    "training_mode = True\n",
    "training_dynamic = True\n",
    "training_reward = False\n",
    "load_dynamic_replay_buffer = False\n",
    "training_warm_up_phase = False\n",
    "discriminator_warm_up_phase = False\n",
    "load_expert_policy = False\n",
    "logger_start_state = 0 if INITIALIZE_TRAINING_INSTANCE else 0\n",
    "dynamic_start_state = 0 if INITIALIZE_TRAINING_INSTANCE else 0\n",
    "# endregion\n",
    "\n",
    "# region file path\n",
    "# path information\n",
    "root_folder                 = \"E:\\MoCAP\\MCDH\\\\root_1\"\n",
    "policy_model_path           = f\"{root_folder}\\\\sac_model_3 - Copy.zip\"\n",
    "dynamic_model_path          = f\"{root_folder}\\\\d1_init.pt\"\n",
    "reward_model_path           = f\"{root_folder}\\\\r1_64.pt\"\n",
    "dynamic_model_backup_path   = f\"{root_folder}\\\\d1_init.pt\"\n",
    "reward_model_backup_path    = f\"{root_folder}\\\\r1_64_backup.pt\"\n",
    "replay_buffer_path          = f\"{root_folder}\\\\replay_buffer.pt\"\n",
    "dynamic_model_replay_buffer = f\"{root_folder}\\dyna_replay_buffer_backup.pt\"\n",
    "logger_path                 = f\"{root_folder}\\logs\"\n",
    "logger2_path                = logger_path\n",
    "reference_trajectory_path   = f\"{root_folder}/traj_info.np\"\n",
    "feature_extractor_info      = f\"{root_folder}/feature_extractor_info.pkl\"\n",
    "# endregion\n",
    "\n",
    "# region algorithm models\n",
    "\n",
    "# tensorboard summary writer\n",
    "writer = SummaryWriter(logger2_path)\n",
    "\n",
    "# expert info\n",
    "dataset = types.ClipCollection(ids=['CMU_075_09'], start_steps=[0], end_steps=[194])\n",
    "\n",
    "# referenct trajectory information\n",
    "with open(reference_trajectory_path, \"rb\") as f: reference_info = pickle.load(f)\n",
    "with open(feature_extractor_info   , \"rb\") as f: fe_info = pickle.load(f)\n",
    "\n",
    "# environment\n",
    "env  = tracking.MocapTrackingGymEnv(dataset, task_kwargs=dict(ghost_offset=np.array([0., 0., 0.])),)\n",
    "denv = DynaBasedEnv(env, reference_info, None, keys=fe_info[0], observation_space=fe_info[1])\n",
    "\n",
    "# policy model\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=3*[1024], qf=3*[1024]),\n",
    "    activation_fn=torch.nn.LeakyReLU,\n",
    ")\n",
    "\n",
    "lr_schedule = 1e-4\n",
    "format_strings = ['csv', 'tensorboard', 'stdout']\n",
    "\n",
    "model = TD3(\"MlpPolicy\", \n",
    "            denv,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            gamma=0.95,\n",
    "            batch_size=4096,\n",
    "            learning_rate=lr_schedule,\n",
    "            verbose=1,\n",
    "            device=get_device(\"cuda\"),)\n",
    "\n",
    "# dynamic model\n",
    "d1 = MultiStepDynamics(NUM_OF_CELLS, hidden_layer_frac=[0.6,0.5], dropout=0.2)\n",
    "\n",
    "# reward model\n",
    "r1 = Discriminator(action_dim=denv.action_space.shape[0], state_dim=denv.observation_space.shape[0], hidden_dim=512, writer=writer)\n",
    "# endregion\n",
    "\n",
    "# region load models\n",
    "# load models\n",
    "if load_reward:\n",
    "    r1.load_state_dict(torch.load(reward_model_path))\n",
    "    print(f\"[INFO] : Reward Loaded Successfully.\")\n",
    "else:\n",
    "    r1 = Discriminator(action_dim=ACTION_SIZE, state_dim=STATE_SIZE, writer=None)\n",
    "    print(f\"[INFO] : Reward Initialized Successfully.\")\n",
    "if load_networks : \n",
    "    custom_objects = { 'learning_rate': lr_schedule, 'batch_size': 8000}\n",
    "    model = TD3.load(policy_model_path, env=denv, custom_objects=custom_objects)\n",
    "    print(f\"[INFO] : Model Loaded Successfully.\")\n",
    "if load_replay_buffer : \n",
    "    model.load_replay_buffer(replay_buffer_path)\n",
    "    print(f\"[INFO] : Buffer Loaded Successfully.\")\n",
    "    print(f\"[INFO] : Buffer Size IS {model.replay_buffer.pos}\")\n",
    "else : \n",
    "    model.replay_buffer.reset()\n",
    "    print(f\"[INFO] : Buffer Reset Was Successfull.\")\n",
    "if load_physics : \n",
    "    d1.load_state_dict(torch.load(dynamic_model_backup_path))\n",
    "    print(f\"[INFO] : Physics Loaded Successfully.\")\n",
    "if load_dynamic_replay_buffer:\n",
    "    with open(dynamic_model_replay_buffer, \"rb\") as f:\n",
    "        dynamic_dataset = pickle.load(f)\n",
    "    print(f\"[INFO] : Physics Buffer Loaded Successfully.\")\n",
    "    print(f\"[INFO] : Physics Buffer Size IS {len(dynamic_dataset[0])}\")\n",
    "else: \n",
    "    dynamic_dataset = [[], []]\n",
    "    print(f\"[INFO] : Physics Buffer Initialized Successfully.\")\n",
    "# endregion\n",
    "\n",
    "# region initialization\n",
    "logger = configure(logger_path, format_strings)\n",
    "model.set_logger(logger)\n",
    "one_tensor = torch.tensor([1.])\n",
    "# _pos_indeces = np.array(list(range(30)))\n",
    "\n",
    "gamma = torch.tensor([0.95**n for n in range(241)]).double()\n",
    "action_gamma = torch.tensor([[x]*ACTION_SIZE for x in gamma[:25]]).reshape(-1)\n",
    "target_bodies = reference_info['body_positions'].reshape(-1,93)\n",
    "target_joints = reference_info['joints'].reshape(-1,ACTION_SIZE)\n",
    "target_reference = np.concatenate((target_bodies,target_joints), axis=1)\n",
    "\n",
    "# _pos_target_trajs = torch.tensor(np.concatenate((target_bodies,target_joints), axis=-1))[:,_pos_indeces]\n",
    "# _ang_target_trajs = torch.tensor(np.concatenate((target_bodies,target_joints), axis=-1))[:,_ang_indeces]\n",
    "# target_trajs = torch.tensor(np.concatenate((target_bodies,target_joints), axis=-1))\n",
    "# endregion\n",
    "\n",
    "# region algorithm functions\n",
    "def r_to_i(x,y): return np.array([z for z in range(3*x,3*y)])\n",
    "\n",
    "def r_norm(x,y): \n",
    "    # print(f\"[DEBUG] : Error matrix = {np.max(np.einsum('i,ji->ji',np.abs(x - y),mask), axis=1)}\")\n",
    "    return np.sum(np.mean(np.einsum(\"i,ji->ji\",np.sqrt(np.square(x - y)+1e-6),mask[:-1]), axis=1))+\\\n",
    "           np.mean(np.einsum(\"i,i->i\",np.sqrt(np.square(x - y)+1e-6),mask[-1]))\n",
    "        \n",
    "def traj_dist(epS, start_step,result_index=-1, traj_length=25, result_array=None, multi_thread=False) : \n",
    "    dtw_distance, _ = fastdtw(epS.reshape(traj_length,POS_SIZE).detach().numpy(),\n",
    "                              target_reference[start_step:start_step+traj_length,:], \n",
    "                              dist=r_norm)\n",
    "    # print(f\"[DEBUG] : DTW distance is {dtw_distance}\")\n",
    "\n",
    "    if not multi_thread : return [dtw_distance, one_tensor]\n",
    "    result_array[result_index][0] = dtw_distance\n",
    "    result_array[result_index][1] = 1.0\n",
    "\n",
    "def predictive_sampling(S0, start_step, predictive_trials=100, temperature=0.1):\n",
    "    model.policy.eval()\n",
    "    actions = torch.tensor([])\n",
    "    current_state = torch.tensor(S0)\n",
    "    # estimate main trajectory\n",
    "    for i in range(25) :\n",
    "        next_action = torch.clamp(torch.tensor(model.predict(current_state.detach().numpy(), deterministic=False)[0]), min=-1.0, max=+1.0)\n",
    "        # next_action = torch.clamp(temperature*torch.tensor(expert.predict(denv.core_state, deterministic=True)[0])+(1-temperature)*torch.empty(ACTION_SIZE).uniform_(-0.05, 0.05), min=-1.0, max=+1.0)\n",
    "        # next_action = torch.clamp(torch.tensor(expert.predict(denv.core_state, deterministic=False)[0]), min=-1.0, max=+1.0)\n",
    "        actions = torch.cat((actions, next_action), dim=-1)\n",
    "        current_state = d1.Nthforward(x=torch.cat((current_state if i==0 else d1.decoder(current_state), next_action), dim=-1), N=i)\n",
    "    # disturb main trajectory\n",
    "    disturbed_actions = torch.clamp(actions + torch.normal(torch.zeros((predictive_trials, int(actions.shape[-1]))), 0.125), min=-1.0, max=+1.0)\n",
    "    disturbed_trajs = torch.cat((torch.tile(current_state, dims=(predictive_trials,1)), disturbed_actions), dim=-1)\n",
    "    # feed trajectories to dynamic model\n",
    "    predicted_trajs = d1(disturbed_trajs)\n",
    "    \n",
    "    # threads = []\n",
    "    global number_of_predicted_trajs\n",
    "    traj_distances = []\n",
    "    for result_index in range(number_of_predicted_trajs) : \n",
    "        traj_distances += [traj_dist(predicted_trajs[result_index,:], start_step,result_index=-1, traj_length=25, multi_thread=False)]\n",
    "\n",
    "\n",
    "    traj_distances = torch.tensor(traj_distances)\n",
    "    # print(f\"[DEBUG] : mean trajectory errors = {torch.mean(traj_distances[:,0])}\")\n",
    "    # estimate errors\n",
    "    return disturbed_actions[torch.argmin(traj_distances[:,0]),:][:ACTION_SIZE], torch.min(traj_distances[:,0])\n",
    "\n",
    "# training loop\n",
    "def training_loop(dataloader, num_epochs, optimizer, writer, partial_loss_counter, ci, device):\n",
    "    for counter in tqdm(range(num_epochs), desc=f\"cell[{ci}]\"):\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            # optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # criterion\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = d1(batch_x.to(device))\n",
    "\n",
    "            # individual loss\n",
    "            loss = criterion(outputs[:,:(ci+1)*STATE_SIZE], batch_y[:,:(ci+1)*STATE_SIZE].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            partial_running_loss[ci] += loss.item()\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "        # LOG THE Partial Cell Loss \n",
    "        writer.add_scalar(f\"Partial Dynamic Loss/cell[{ci}]\", loss.item(), partial_loss_counter+counter)\n",
    "\n",
    "# main dynamic training funtion\n",
    "def training_function(dataset, batch_size, num_epochs, d1, writer, dynamic_start_state, episode_num, device=torch.device(\"cpu\")):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    d1.to(device)\n",
    "    for ci in range(25):\n",
    "        training_loop(dataloader, num_epochs, optim.Adam(d1.cells[:ci+1].parameters(), lr=1e-5), writer, dynamic_start_state+episode_num*25, ci, device)\n",
    "\n",
    "# endregion\n",
    "\n",
    "# region algorithm parameters\n",
    "\n",
    "# region main loop params\n",
    "NUM_TRAJS = 25\n",
    "NUM_ITERS = 1000 if not TRAIN_DYNAMICS_INITIALIZATION_ONLY else 0\n",
    "number_of_predicted_trajs = 100\n",
    "counter = 0\n",
    "__total_timesteps = 1000\n",
    "# endregion\n",
    "\n",
    "# region model params\n",
    "entropy_coef = 0.7\n",
    "partial_running_loss = [0.0 for _ in range(25)]\n",
    "batch_size = 1200\n",
    "zero_array = np.zeros((1), dtype=int)\n",
    "Z = [np.zeros((ACTION_SIZE,)) for _ in range(25)]\n",
    "replay_buffer_store_period = 5\n",
    "model_store_period = 1\n",
    "device = torch.device(\"cpu\")\n",
    "# endregion\n",
    "\n",
    "# region dynamic params\n",
    "dynamic_batch_size = 1240\n",
    "dynamic_train_iterations = 2\n",
    "dynamic_writer_counter = 0\n",
    "num_epochs = 2\n",
    "dynamic_training_period = 1\n",
    "# endregion\n",
    "\n",
    "# region reward params\n",
    "expert_buffer = [[],[]]\n",
    "reward_train_epoch_size = 1024\n",
    "reward_train_batch_size = 512\n",
    "reward_train_num_of_epochs = 5\n",
    "reward_train_learning_rate = 1e-4\n",
    "reward_train_iterations = 20\n",
    "mask = np.zeros((7,POS_SIZE))\n",
    "\n",
    "# _pos_indeces = np.array([list(range(3*x,3*x+3,1)) for x in [2,3,7,8,19,22,26,29,10,16]]).reshape(-1)\n",
    "_ang_indeces = 93+np.array([1,2,3,4,5,6,8,9,10,11,12,13,33,34,38,45,46,50]).reshape(-1)\n",
    "\n",
    "pos_coef = 0.5\n",
    "mask[0, [\n",
    "    3 * 1 + 2,\n",
    "    3 * 6 + 2,\n",
    "]] = pos_coef\n",
    "mask[1, [\n",
    "    3 * 4 + 2,\n",
    "    3 * 9 + 2,\n",
    "]] = pos_coef\n",
    "mask[2, [\n",
    "    3 * 2 + 2,\n",
    "    3 * 7 + 2,\n",
    "]] = pos_coef\n",
    "mask[3, [\n",
    "    3 * 3 + 2,\n",
    "    3 * 8 + 2,\n",
    "]] = pos_coef\n",
    "mask[4, [\n",
    "    36, 37, 38,\n",
    "]] = pos_coef\n",
    "mask[5, [\n",
    "    45, 46, 47,\n",
    "]] = pos_coef\n",
    "mask[6,  _ang_indeces,] = .3\n",
    "\n",
    "if not load_reward: r1.writer = writer\n",
    "denv.reward_estimator = r1\n",
    "print(\"[INFO] : Environment Reward Estimator Was Set Successfully.\")\n",
    "# endregion\n",
    "\n",
    "# endregion\n",
    "\n",
    "raise Exception(\"STOP HERE\")\n",
    "\n",
    "\n",
    "# region algorithm main loop\n",
    "if discriminator_warm_up_phase:\n",
    "    assert load_replay_buffer is True, \"[WARNING] : Make Sure Replay Buffer Is Initiated In Warming Reward Phase\"\n",
    "    for _ in tqdm(list(range(20))):\n",
    "        expert_data,agent_data = model.replay_buffer.sample(batch_size=4096,reward_training=True)\n",
    "        r1.train(expert_data, agent_data, num_epochs=10, batch_size=512, lr=1e-4)\n",
    "\n",
    "if training_warm_up_phase:\n",
    "    print(f\"[INFO] : Warmup Phase Activated...\")\n",
    "    # model.train(gradient_steps=15,batch_size=4096)\n",
    "    model.learn( total_timesteps=__total_timesteps, reset_num_timesteps=False )\n",
    "\n",
    "for episode_num in range(NUM_ITERS):\n",
    "\n",
    "    # Collect Trajectory\n",
    "    reward_stats, length_stats = [], []\n",
    "    dtw_stats, mse_stats= [], []\n",
    "    for traj_num in tqdm(range(NUM_TRAJS)):\n",
    "\n",
    "        # recording video of robot performance\n",
    "        _current_eps_frames = []\n",
    "\n",
    "        n_state, done = denv.reset(ps_mode=True)\n",
    "        real_states, pos_states, actions = [], [], []\n",
    "        episode_start = copy.deepcopy(denv.main_env.dm_env._task._time_step)\n",
    "        traj_info = []\n",
    "        while not done:\n",
    "            # capture current frame\n",
    "            _current_eps_frames += [denv.render()]\n",
    "            _current_time = denv.main_env.dm_env._task._time_step\n",
    "            action, traj_error = predictive_sampling(n_state[0], _current_time,predictive_trials=number_of_predicted_trajs, temperature=0.9)\n",
    "            # action, traj_error = torch.clamp(torch.tensor(expert.predict(denv.core_state, deterministic=False)[0]), min=-1.0, max=+1.0), 0.1\n",
    "            # print(f\"[DEBUG] : Trajectory error = {traj_error}\")\n",
    "            state = np.copy(n_state[0])\n",
    "            real_states += [n_state[0]]\n",
    "            pos_states += [n_state[1]]\n",
    "            actions += [action]\n",
    "            n_state, reward, done, _, _ = denv.step(action, ps_mode=True)\n",
    "            \n",
    "            # store to train expert reward estimator\n",
    "            expert_buffer[0] += [n_state[0].reshape((1,-1))]\n",
    "            expert_buffer[1] += [action.reshape((1,-1))]\n",
    "\n",
    "            # store to train \n",
    "            traj_info += [\n",
    "                (\n",
    "                    action.detach().numpy().reshape((1,-1)),\n",
    "                    zero_array,\n",
    "                    n_state[0].reshape((1,-1)),\n",
    "                    state[0].reshape((1,-1)),\n",
    "                    reward.reshape((1,-1)),\n",
    "                    [{\"TimeLimit.truncated\" : False}],\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        for step_info in traj_info:\n",
    "            model.replay_buffer.add(\n",
    "                action   =step_info[0],\n",
    "                done     =step_info[1],\n",
    "                next_obs =step_info[2],\n",
    "                obs      =step_info[3],\n",
    "                reward   =step_info[4]-1/len(traj_info),\n",
    "                infos    =step_info[5],\n",
    "            )\n",
    "\n",
    "        # truncate the expert reward estimator buffer \n",
    "        expert_buffer[0] = expert_buffer[0][-4000:]\n",
    "        expert_buffer[1] = expert_buffer[1][-4000:]\n",
    "\n",
    "\n",
    "        # Update Dynamics Dataset\n",
    "        real_states += [n_state[0]]\n",
    "        pos_states += [n_state[1]]\n",
    "        if len(real_states) >= 25:\n",
    "            for start in range(len(real_states)-25+1-1):\n",
    "                dynamic_dataset[0] += [ np.concatenate([real_states[start]]+actions[start:start+25], axis=-1) ]\n",
    "                dynamic_dataset[1] += [ np.concatenate(pos_states[start+1:start+1+25], axis=-1) ]\n",
    "        else:\n",
    "            for start in range(len(real_states)):\n",
    "                dynamic_dataset[0] += [ np.concatenate([real_states[start]]+actions[start:]+Z[:25-len(actions[start:])], axis=-1) ]\n",
    "                dynamic_dataset[1] += [ np.concatenate(pos_states[start+1:]+[pos_states[-1] for _ in range(25-len(pos_states[start+1:]))], axis=-1) ]\n",
    "        \n",
    "        # update episode reward stats\n",
    "        best_traj_error = traj_dist(epS=torch.tensor(pos_states), result_index=-1, start_step=episode_start, traj_length=len(pos_states))\n",
    "        dtw_stats += [ best_traj_error[0] ]\n",
    "        length_stats += [ len(pos_states) ]\n",
    "\n",
    "        # store performance video\n",
    "        out = cv2.VideoWriter(f'{logger2_path}\\\\videos\\\\episode-[{episode_num}]-[{traj_num}].mp4',cv2.VideoWriter_fourcc(*'DIVX'),15, (640, 480))\n",
    "        for i in range(len(_current_eps_frames)):\n",
    "            rgb_img = cv2.cvtColor(_current_eps_frames[i], cv2.COLOR_RGB2BGR)\n",
    "            out.write(rgb_img)\n",
    "        out.release()\n",
    "\n",
    "\n",
    "    # Sanity Checks\n",
    "    print(f\"[INFO] : Model Buffer Current POS IS {model.replay_buffer.pos}\")\n",
    "    print(f\"[INFO] : Dynamic Buffer Size IS {len(dynamic_dataset[0])}\")\n",
    "\n",
    "\n",
    "    # Train The Dynamic Model\n",
    "    if training_dynamic:\n",
    "        for _ in tqdm(range(dynamic_train_iterations)):\n",
    "            dynamic_dataset[0], dynamic_dataset[1] = dynamic_dataset[0][-4000:], dynamic_dataset[1][-4000:]\n",
    "            indeces = random.sample(range(len(dynamic_dataset[1])), min(len(dynamic_dataset[1]), dynamic_batch_size))\n",
    "            dataset = CustomDataset([dynamic_dataset[0][__i] for __i in indeces], [dynamic_dataset[1][__i] for __i in indeces], device=torch.device(\"cuda\"))\n",
    "            training_function(dataset, batch_size, num_epochs, d1, writer, dynamic_start_state, dynamic_writer_counter)\n",
    "            dynamic_writer_counter += 1\n",
    "       \n",
    "\n",
    "    # Collect Extra Steps For Algorithm Itself\n",
    "    if training_mode:\n",
    "        model.learn( total_timesteps=int(__total_timesteps), reset_num_timesteps=False, log_interval=10)\n",
    "        \n",
    "\n",
    "    # Train The Reward Model\n",
    "    if training_reward:\n",
    "        for _ in tqdm(list(range(reward_train_iterations))):\n",
    "            _agent_data = model.replay_buffer.sample(batch_size=reward_train_epoch_size)\n",
    "            agent_data = [_agent_data[0].detach().cpu().numpy(), _agent_data[1].detach().cpu().numpy()]\n",
    "            indeces = random.sample(range(len(expert_buffer[0])), min(reward_train_epoch_size,len(expert_buffer[0])))\n",
    "            expert_data = [np.array(expert_buffer[0]).squeeze()[indeces], np.array(expert_buffer[1]).squeeze()[indeces]]\n",
    "            r1.train(expert_data, agent_data, num_epochs=reward_train_num_of_epochs, batch_size=reward_train_batch_size, lr=reward_train_learning_rate)\n",
    "\n",
    "    # check point every 5 steps\n",
    "    if (episode_num%model_store_period==0) and ((episode_num>=0) or (replay_buffer_store_period==1)):\n",
    "\n",
    "        # backup model\n",
    "        if training_mode:\n",
    "            model.save(policy_model_path)\n",
    "            print(f\"[INFO] : Model Stored Successfully.\")\n",
    "\n",
    "        # dynamic model\n",
    "        if training_dynamic:\n",
    "            torch.save(d1.state_dict(), dynamic_model_backup_path)\n",
    "            print(f\"[INFO] : Dynamics Model Stored Successfully.\")\n",
    "\n",
    "        # reward model\n",
    "        if training_reward:\n",
    "            torch.save(r1.state_dict(), reward_model_backup_path)\n",
    "            print(f\"[INFO] : Reward Model Stored Successfully.\")\n",
    "\n",
    "\n",
    "    # store & truncate replay buffer\n",
    "    if (episode_num%replay_buffer_store_period==0) and ((episode_num>0) or (replay_buffer_store_period==1)) and training_mode: \n",
    "        \n",
    "        # model replay buffer\n",
    "        model.save_replay_buffer(replay_buffer_path)\n",
    "        print(f\"[INFO] : Model Replay Buffer Stored Successfully.\")\n",
    "\n",
    "        # dynamic replay buffer\n",
    "        with open(dynamic_model_replay_buffer, \"wb\") as f:\n",
    "            pickle.dump(file=f, obj=dynamic_dataset)\n",
    "            print(f\"[INFO] : Dynamics Replay Buffer Stored Successfully.\")\n",
    "# endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect trajectories to train gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10000/10000 [46:32<00:00,  3.58it/s] \n"
     ]
    }
   ],
   "source": [
    "# Collect Trajectory\n",
    "# sequence_buffer = [] # containes trajectories of form state[206]-action[56] pairs , [[S1],[A1],...,[ST-1],[AT-1],[ST]]\n",
    "state_buffer = [] # containes pairs of states and their corresponding position collected from trajectoreis\n",
    "for traj_num in tqdm(range(10000)):\n",
    "\n",
    "    # recording video of robot performance\n",
    "    n_state, done = denv.reset(ps_mode=True)\n",
    "    # sequence_buffer += [[]]\n",
    "    while not done:\n",
    "        # capture current frame\n",
    "        # action = torch.clamp(torch.tensor(expert.predict(denv.core_state, deterministic=False)[0]), min=-1.0, max=+1.0)\n",
    "        action = torch.clamp(torch.tensor(expert.predict(denv.core_state, deterministic=True)[0])+torch.empty(ACTION_SIZE).uniform_(-0.75, 0.75), min=-1.0, max=+1.0)\n",
    "\n",
    "        # store state action pair\n",
    "        # sequence_buffer[-1] += [\n",
    "        #     [n_state[0]],\n",
    "        #     [action.detach().cpu().numpy()],\n",
    "        # ]\n",
    "\n",
    "        state_buffer += [\n",
    "            n_state\n",
    "        ]\n",
    "        \n",
    "        n_state, reward, done, _, _ = denv.step(action, ps_mode=True)\n",
    "        # store final state if done\n",
    "        if done: \n",
    "            # sequence_buffer[-1] += [\n",
    "            #     [n_state[0]],\n",
    "            # ]\n",
    "            state_buffer += [ n_state ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115705"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"E:/MoCAP/MCDH/minGPT/test_trajs.pkl\", \"wb\") as f: pickle.dump(sequence_buffer, f)\n",
    "with open(\"E:/MoCAP/MCDH/StPosAE/train_stpos.pkl\", \"wb\") as f: pickle.dump(state_buffer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
