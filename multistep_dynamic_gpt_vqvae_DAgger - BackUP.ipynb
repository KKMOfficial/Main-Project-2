{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/microsoft/MoCapAct.git\n",
    "%cd MoCapAct\n",
    "!pip install -e .\n",
    "%cd /content/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\4_Installed_Softwares\\mamba_forge\\lib\\site-packages\\gym\\envs\\registration.py:250: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for plugin in metadata.entry_points().get(entry_point, []):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.26.5, Python 3.10.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\4_Installed_Softwares\\mamba_forge\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "e:\\4_Installed_Softwares\\mamba_forge\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import copy, cv2, pickle, torch, random\n",
    "from tqdm import tqdm\n",
    "from dm_control.locomotion.tasks.reference_pose import types\n",
    "from dm_control.locomotion.tasks.reference_pose import utils as _utils\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from mocapact import observables\n",
    "from mocapact.sb3 import utils\n",
    "from mocapact.envs import tracking\n",
    "\n",
    "from stable_baselines3 import TD3, SAC\n",
    "# from sac_modified import SAC\n",
    "\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import get_device\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Optional\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import chebyshev\n",
    "\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dynamic gpt model\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
    "\n",
    "# vqvae model\n",
    "from vqvae import VQVAE\n",
    "from quantizer import get_quantizer\n",
    "\n",
    "# stpos model\n",
    "from stpos import StPosAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Dynamic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VQ-VAE Loading] : Successful.\n",
      "[StPos Loading] : Successful.\n",
      "device: cuda\n",
      "system:\n",
      "    seed: 3407\n",
      "    work_dir: E:/MoCAP/MCDH/model_based_planner/Experiment_Inverse_Reward_without_priorotized_replay/out/dynamic\n",
      "model:\n",
      "    model_type: None\n",
      "    n_layer: 2\n",
      "    n_head: 2\n",
      "    n_embd: 512\n",
      "    vocab_size: 401\n",
      "    block_size: 400\n",
      "    embd_pdrop: 0.1\n",
      "    resid_pdrop: 0.1\n",
      "    attn_pdrop: 0.1\n",
      "trainer:\n",
      "    device: cuda\n",
      "    num_workers: 4\n",
      "    max_iters: None\n",
      "    batch_size: 2\n",
      "    learning_rate: 0.0001\n",
      "    betas: (0.9, 0.95)\n",
      "    weight_decay: 0.1\n",
      "    grad_norm_clip: 1.0\n",
      "\n",
      "number of parameters: 6.72M\n",
      "[GPT loading] : Successful.\n"
     ]
    }
   ],
   "source": [
    "def get_config():\n",
    "\n",
    "    C = CN()\n",
    "\n",
    "    # device\n",
    "    C.device = 'cuda'\n",
    "\n",
    "    # system\n",
    "    C.system = CN()\n",
    "    C.system.seed = 3407\n",
    "    C.system.work_dir = 'E:/MoCAP/MCDH/model_based_planner/Experiment_Inverse_Reward_without_priorotized_replay/out/dynamic'\n",
    "\n",
    "    # model\n",
    "    C.model = GPT.get_default_config()\n",
    "    C.model.model_type = 'gpt-nano'\n",
    "\n",
    "    # trainer\n",
    "    C.trainer = Trainer.get_default_config()\n",
    "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "    return C\n",
    "\n",
    "# get default config and overrides from the command line, if any\n",
    "root_dir = \"E:/MoCAP/MCDH/model_based_planner/Experiment_Inverse_Reward_without_priorotized_replay\"\n",
    "config = get_config()\n",
    "config.trainer.device='cuda'\n",
    "# config.trainer.batch_size = 2\n",
    "config.model.model_type = None\n",
    "config.model.n_layer = 2\n",
    "config.model.n_head = 2\n",
    "config.model.n_embd = 512\n",
    "config.trainer.learning_rate = 1e-4\n",
    "# config.trainer.num_workers = 0 # windows only\n",
    "# config.merge_from_args(sys.argv[1:])\n",
    "setup_logging(config)\n",
    "set_seed(config.system.seed)\n",
    "\n",
    "# load the VQ-VAE model\n",
    "vqvae_model = VQVAE(input_dim=1, hidden_dim=44, num_embeddings=200, embedding_dim=44)\n",
    "try:\n",
    "    vqvae_model.load_state_dict(torch.load(f\"{root_dir}/vqvae_check_point_44_200.pt\"))\n",
    "    print('[VQ-VAE Loading] : Successful.')\n",
    "except Exception as e:\n",
    "    print(f'[VQ-VAE Loading] : Failed, {e}')\n",
    "\n",
    "# load StPos model\n",
    "stpos_model = StPosAE(input_ch=1, hidden_ch=150, x_dim=206, z_dim=149)\n",
    "try:\n",
    "    print(f\"[StPos Loading] : Successful.\")\n",
    "    stpos_model.load_state_dict(torch.load(f\"{root_dir}/stpos_150_206_149.pt\"))\n",
    "except Exception as e:\n",
    "    print(f\"[StPos Loading] : Failed, {e}\")\n",
    "\n",
    "\n",
    "# construct the training dataset\n",
    "dynamic_actions = 5\n",
    "\n",
    "# construct the model\n",
    "config.model.vocab_size = 200 + 200 + 1\n",
    "config.model.block_size = dynamic_actions * (24 + 56)\n",
    "\n",
    "# print dynamic configs\n",
    "print(config)\n",
    "\n",
    "# load model's weights using checkpoint\n",
    "gpt_model = GPT(config.model)\n",
    "try:\n",
    "    gpt_model.load_state_dict(torch.load(f\"{root_dir}/dynamic_gpt_2_2_512_400_401.pt\"))\n",
    "    print(f'[GPT loading] : Successful.')\n",
    "except Exception as e:\n",
    "    print(f\"[GPT loading] : Faild. {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region Dynamic Model Parameters\n",
    "# ps_mode : state=[real_state, pos_state]\n",
    "# DYNAMIC : STATE_SIZE,25*ACTION_SIZE -> 25*POS_SIZE\n",
    "STATE_SIZE = 206\n",
    "POS_SIZE = 149\n",
    "ACTION_SIZE = 56\n",
    "NUM_OF_CELLS = 25\n",
    "# endregion\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "\n",
    "class DynaBasedEnv(gymnasium.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "    def __init__(\n",
    "            self, \n",
    "            main_env,\n",
    "            expert_traj,\n",
    "            keys = None,\n",
    "            observation_space = None,\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.main_env = main_env\n",
    "        self.expert_traj = expert_traj\n",
    "        self.action_space = spaces.Box(low=float('-1'), high=float('1'), shape=(ACTION_SIZE,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=float('-inf'), high=float('inf'), shape=(STATE_SIZE,), dtype=np.float32)\n",
    "        self.core_state = None\n",
    "        self._keys = keys\n",
    "        self._observation_space = observation_space\n",
    "\n",
    "    def transform_observation(self, observations):\n",
    "        obs = []\n",
    "        for k in self._keys:\n",
    "            if k in observations:\n",
    "                obs.append(observations[k])\n",
    "            else:\n",
    "                tmp = list(observations.values())[0]\n",
    "                shape = list(tmp.shape)\n",
    "                shape[-1] = self._observation_space[k].shape[0]\n",
    "                obs.append(torch.full(shape, torch.nan, device=tmp.device))\n",
    "        obs = np.concatenate(obs, axis=-1)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        self.core_state, rew, done, _ = self.main_env.step(action)\n",
    "        return self.transform_observation(self.core_state), rew, done, False, {\"TimeLimit.truncated\":None}\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.core_state, done = self.main_env.reset(), False\n",
    "        return self.transform_observation(self.core_state), {\"TimeLimit.truncated\":None}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        return self.main_env.render(\"rgb_array\")\n",
    "          \n",
    "    def close(self):\n",
    "        self.main_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:23: DeprecationWarning: invalid escape sequence '\\M'\n",
      "<>:23: DeprecationWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_14136\\3301753910.py:23: DeprecationWarning: invalid escape sequence '\\M'\n",
      "  root_folder                 = \"E:\\MoCAP\\MCDH\\\\root_1\"\n"
     ]
    }
   ],
   "source": [
    "# region Important Notes\n",
    "# most important parameters to set\n",
    "# training rate 3e-4\n",
    "# depth and width of the value and policy networks 3*1024\n",
    "# entropy coefficient 0.7\n",
    "# make the buffer size 100K\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# endregion\n",
    "\n",
    "# region control flags\n",
    "# training initialization or storing\n",
    "INITIALIZE_TRAINING_INSTANCE = False\n",
    "load_networks = True\n",
    "training_mode = True\n",
    "training_warm_up_phase = False\n",
    "load_expert_policy = False\n",
    "logger_start_state = 0 if INITIALIZE_TRAINING_INSTANCE else 0\n",
    "# endregion\n",
    "\n",
    "# region file path\n",
    "# path information\n",
    "root_folder                 = \"E:\\MoCAP\\MCDH\\\\root_1\"\n",
    "reference_trajectory_path   = f\"{root_folder}/traj_info.np\"\n",
    "feature_extractor_info      = f\"{root_folder}/feature_extractor_info.pkl\"\n",
    "# endregion\n",
    "\n",
    "dataset = types.ClipCollection(ids=['CMU_075_09'], start_steps=[0], end_steps=[194])\n",
    "\n",
    "# referenct trajectory information\n",
    "with open(reference_trajectory_path, \"rb\") as f: reference_info = pickle.load(f)\n",
    "with open(feature_extractor_info   , \"rb\") as f: fe_info = pickle.load(f)\n",
    "\n",
    "# environment\n",
    "\n",
    "# policy model\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=3*[512], qf=3*[512]),\n",
    "    activation_fn=torch.nn.LeakyReLU,\n",
    ")\n",
    "\n",
    "lr_schedule = 1e-4\n",
    "format_strings = ['csv', 'tensorboard', 'stdout']\n",
    "\n",
    "\n",
    "one_tensor = torch.tensor([1.])\n",
    "target_bodies = reference_info['body_positions'].reshape(-1,93)\n",
    "target_joints = reference_info['joints'].reshape(-1,ACTION_SIZE)\n",
    "target_reference = np.concatenate((target_bodies,target_joints), axis=1)\n",
    "\n",
    "# region algorithm functions\n",
    "def r_to_i(x,y): return np.array([z for z in range(3*x,3*y)])\n",
    "\n",
    "def r_norm(x,y): \n",
    "    return np.sum(np.mean(np.einsum(\"i,ji->ji\",np.sqrt(np.square(x - y)+1e-6),mask[:-1]), axis=1))+\\\n",
    "           np.mean(np.einsum(\"i,i->i\",np.sqrt(np.square(x - y)+1e-6),mask[-1]))\n",
    "        \n",
    "def traj_dist(epS, start_step,result_index=-1, traj_length=25, result_array=None, multi_thread=False) : \n",
    "    dtw_distance, _ = fastdtw(epS.reshape(traj_length,POS_SIZE).cpu().detach().numpy(),\n",
    "                              target_reference[start_step:start_step+traj_length,:], \n",
    "                              dist=r_norm)\n",
    "    # print(f\"[DEBUG] : DTW distance is {dtw_distance}\")\n",
    "\n",
    "    return dtw_distance\n",
    "\n",
    "\n",
    "# endregion\n",
    "\n",
    "# region algorithm parameters\n",
    "\n",
    "# region main loop params\n",
    "NUM_TRAJS = 25\n",
    "NUM_ITERS = 1000\n",
    "number_of_predicted_trajs = 100\n",
    "# endregion\n",
    "\n",
    "\n",
    "mask = np.zeros((7,POS_SIZE))\n",
    "# _pos_indeces = np.array([list(range(3*x,3*x+3,1)) for x in [2,3,7,8,19,22,26,29,10,16]]).reshape(-1)\n",
    "_ang_indeces = 93+np.array([1,2,3,4,5,6,8,9,10,11,12,13,33,34,38,45,46,50]).reshape(-1)\n",
    "\n",
    "pos_coef = 0.5\n",
    "mask[0, [\n",
    "    3 * 1 + 2,\n",
    "    3 * 6 + 2,\n",
    "]] = pos_coef\n",
    "mask[1, [\n",
    "    3 * 4 + 2,\n",
    "    3 * 9 + 2,\n",
    "]] = pos_coef\n",
    "mask[2, [\n",
    "    3 * 2 + 2,\n",
    "    3 * 7 + 2,\n",
    "]] = pos_coef\n",
    "mask[3, [\n",
    "    3 * 3 + 2,\n",
    "    3 * 8 + 2,\n",
    "]] = pos_coef\n",
    "mask[4, [\n",
    "    36, 37, 38,\n",
    "]] = pos_coef\n",
    "mask[5, [\n",
    "    45, 46, 47,\n",
    "]] = pos_coef\n",
    "mask[6,  _ang_indeces,] = .3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class policy_net(nn.Module):\n",
    "    def __init__(self, policy_model, dynamic_model, vqvae_model, stpos_model, action_encoder, action_decoder, device=torch.device(\"cpu\")):\n",
    "        super(policy_net, self).__init__()\n",
    "        self.policy    = policy_model.to(device)\n",
    "        self.dynamic   = dynamic_model.to(device)\n",
    "        self.vqvae     = vqvae_model.to(device)\n",
    "        self.transform = stpos_model.to(device)\n",
    "        self.act_en    = action_encoder\n",
    "        self.act_dc    = action_decoder\n",
    "        self.device    = device\n",
    "\n",
    "    def forward(self, x): \n",
    "        with torch.no_grad(): \n",
    "            return self.predictive_sampling(x)\n",
    "    \n",
    "    def predictive_sampling(self,S0,n_actions=10, n_trajs=10, sigma=0.125):\n",
    "        # TODO don't forget to add the return value!!!\n",
    "        # initial actions\n",
    "        self.policy.eval()\n",
    "        _,trial_actions,trial_states = self.dynamic.generate(\n",
    "            idx=S0,\n",
    "            n_actions=n_actions,\n",
    "            actor_network=self.policy, \n",
    "            vqvae=self.vqvae, \n",
    "            act_encoder=self.act_en, \n",
    "            act_decoder=self.act_dc,\n",
    "            perturbed_trajs=n_trajs,\n",
    "            mean=0.0,\n",
    "            sigma=sigma,\n",
    "            device=self.device,\n",
    "        )\n",
    "        transformed_states = self.transform(trial_states.reshape(-1,trial_states.shape[-1]).unsqueeze(1)).reshape((-1,n_actions,149))\n",
    "        # estimated rewards\n",
    "        traj_distances = []\n",
    "        for result_index in range(transformed_states.shape[0]): \n",
    "            traj_distances += [\n",
    "                traj_dist(\n",
    "                    transformed_states.reshape(S0.shape[0]*n_trajs,-1)[result_index],\n",
    "                    start_step=int(round(194*S0[result_index//n_trajs,206-1].item())),\n",
    "                    result_index=-1,\n",
    "                    traj_length=n_actions,)\n",
    "            ]\n",
    "        # best trajectory\n",
    "        ind = torch.tensor(traj_distances).reshape(-1,n_trajs).argmin(1)\n",
    "        return trial_actions[:,0,:][ind + n_trajs*torch.arange(ind.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAgger Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from imitation.util.util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import tempfile\n",
    "from imitation.algorithms import bc\n",
    "from imitation.algorithms.dagger import SimpleDAggerTrainer\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.ddpg.policies import MlpPolicy\n",
    "\n",
    "# referenct trajectory information\n",
    "root_folder                = \"E:\\MoCAP\\MCDH\\\\root_1\"\n",
    "reference_trajectory_path  = f\"{root_folder}/traj_info.np\"\n",
    "feature_extractor_info     = f\"{root_folder}/feature_extractor_info.pkl\"\n",
    "with open(reference_trajectory_path, \"rb\") as f: reference_info = pickle.load(f)\n",
    "with open(feature_extractor_info   , \"rb\") as f: fe_info = pickle.load(f)\n",
    "\n",
    "env  = tracking.MocapTrackingGymEnv(dataset, task_kwargs=dict(ghost_offset=np.array([0., 0., 0.])),)\n",
    "denv = DynaBasedEnv(env, reference_info, keys=fe_info[0], observation_space=fe_info[1])\n",
    "vec_env = DummyVecEnv([lambda: denv for _ in range(1)])\n",
    "\n",
    "expert = DDPG(MlpPolicy, denv, verbose=1, device=torch.device(\"cuda\"))\n",
    "expert_policy = policy_net(\n",
    "    policy_model=expert.policy,\n",
    "    dynamic_model=gpt_model,\n",
    "    vqvae_model=vqvae_model,\n",
    "    stpos_model=stpos_model,\n",
    "    action_encoder=lambda x: (100*x+300)//1,\n",
    "    action_decoder=lambda z: (z-300)/100,\n",
    "    device=torch.device(\"cuda\")\n",
    ")\n",
    "\n",
    "next_action = expert_policy(\n",
    "    torch.rand((10, 206)).to(torch.device(\"cuda\"))\n",
    ")\n",
    "\n",
    "\n",
    "# bc_trainer = bc.BC(\n",
    "#     observation_space=vec_env.observation_space,\n",
    "#     action_space=vec_env.action_space,\n",
    "#     rng=np.random.default_rng(),\n",
    "#     device=torch.device(\"cpu\"),\n",
    "# )\n",
    "# with tempfile.TemporaryDirectory(prefix=\"dagger_example_\") as tmpdir:\n",
    "#     print(tmpdir)\n",
    "#     dagger_trainer = SimpleDAggerTrainer(\n",
    "#         venv=vec_env,\n",
    "#         scratch_dir=tmpdir,\n",
    "#         expert_policy=expert,\n",
    "#         bc_trainer=bc_trainer,\n",
    "#         rng=np.random.default_rng(),\n",
    "#     )\n",
    "\n",
    "#     dagger_trainer.train(2000, rollout_round_min_timesteps=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
